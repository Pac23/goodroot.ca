---
title: "Your Infrastructure Needs a Consul-ar"
author: Kellen Evan Person
date: '2018-04-24'
categories:
  - Culture
slug: consul
published: true
---

# Your Infrastructure Needs a Consul-ar

> Consular. An official appointed by a government to reside in a foreign country to represent the commercial interests of citizens of the appointing country.

In the world of distributed web architecture, there are good ideas and then there are things that _sound_ like good ideas. Microservice architecture sounds like a good idea. But the devil is in the details. What sort of architecture are you transitioning from? What existing tools do you use? What sort of demand is being placed upon your systems? How is your team structured? You might be making a wise decision. Or, you might be setting yourself up for a fruitless hike of human and technical complexity.

## Monolithic Achievements
Architecture is considered monolithic when it weaves together every function into one tightly coupled system. The presentation and user-interface, business logic, database drivers, and third-party services, all roll together into one large deployment. 

If you are just getting started then the monolithic approach is tough to beat. There are some nice cross-cutting concerns like logging, rate limiting, and security, that are easy to handle when you work with one entity.

Monolithic architecture is simple to develop. It can even be performant due to using speedier Inter-Process Communication by default instead of cross-network connections. Early horizontal scale is simple, too — put a load balancer in-front of your applications and add duplicate instances to meet demand as it appears. However, things soon become cumbersome.

Each time a change is made to the deployment, the build/test process occurs to forward the changes to production. This creates blocking; changes can become stuck behind one another as the ever-inflating system begins to increase in complexity. Builds start taking longer. Test coverage becomes its own abstracted web of complication. A change that should be trivial leads to spelunking into thorny, inter-twined services. Developer velocity, over time, slows down.

When it comes to scale, replicating one deployment over and over is inefficient. What if only one component is over-loaded? How much are you spending on extra compute to run every component? 

And how are you managing your load balancers? Have you made them redundant? Are you hard-coding your application addresses into your load balancer when instances fail, partially fail, or are added to the pool?

Now, consider monitoring your monolith. If you use Sensu, for example, then each of your deployments would contain an agent and each agent would call back to the Sensu server. It is elegant, but your logging server that aggregates all of the messages becomes a single point of failure, another piece to manage. If you decide to use Nagios, you would need to update it each time a server is added so that it knows where to send its pings. Again, it is another piece to manage.

A popular new pattern has emerged to quell some of these concerns. Break the monolith!… Into microservices. But is this a wise course of action?


## Microservitude

Consider that you are crafting a web-based game… 

You might have o one service representing your ‘High Score’ board, one for your game logic, one for your UI, one for each chat, users, and library — they can represent resources, functions, data, whatever makes sense for your application. If you are a retail giant like Amazon you might even have more than 100 different services strung together to form a single product page. 

“Micro” services operate like to operate as disparate APIs. Just as a standard REST API, a service must agree to a contract with its clients. If the service receives a request, it must return a certain response. This frees the service up to use whatever technologies it needs to accomplish its functionality and send the expected response. In this way, concerns and contexts are de-coupled.

With Microservices, the deployment process is much more expedient. Each service can be deployed on its own, without a developer needing to re-build the entire architecture. Smaller deployment size leads to shorter build times and less over-lapping test coverage. Developer velocity improves. With the ability to use whatever language or framework under-the-hood, developer happiness might improve, too!

The smaller-service pattern also enables polyglot persistence. You can use the _right_ datastore for the work required, per service — maybe it’s Redis, instead of PostgresSQL, or MongoDB instead of either of them. The service can pick the right tool for its own needs.

Fantastic, on paper. But there are many considerations. We can melt them all together into three key issues…

* How can you connect to, add, and remove new services?
* How do you monitor for, and handle, failures, resurrections, and partial failures?
* How do you make services aware of each other, even across different data centres, and/or regional instances?

## The Trials of Service Management
When new services appear, in most cases, their network location will be dynamic. In addition, due to retiring, scaling, failing, and upgrading, the overall state will often change. The bigger the infrastructure, the more services there are, the more chaos there is to manage. 

### Discovery

Service discovery is the umbrella used to encapsulate this domain of complexity. The process can be led by the client or by a separate server.

Client-side discovery has the client making direct contact with the service registry.  With this approach, the client interacts with the service registry and then it decides how to distribute load across available instances.

Server-side discovery has a load balancer sitting ahead of the service registry. The client reaches the load balancer, the balancer queries the registry, then routes requests to the right, available service. With this method, the client has no awareness of the services. Some might call the client “dumb”, but let us say “less aware”.

Often times, these approaches are combined. The Elastic Load Balancer in AWS, for example, registers and balances traffic to new application instances as they appear.  But in most cases, you have to choose whether… 

* You want an aware client and to manage a highly available service registry.
* You want a less aware client and to manage a highly available service registry _and_ load balancer. 

In both cases, it seems we need a service registry. Building this component is where things start to get _really_ interesting.

### Timely Registration

There are many different approaches that one can take to service registration. You can build your own service registration and discovery mechanisms on top of clever K/V systems like ZooKeeper and etcd. 

In the case of these K/V systems, their core functionality is to keep data consistent and elect leaders across distributed systems in a tolerant way.  A list of active services and their location is one such Key-Value pair that can form the basis for globally service-aware infrastructure. But, the registration feature themselves, writing those keys, must be built and maintained by your developers; it is not part of their native product.

Kubernetes and AWS ELB are two example utilities that have service registry baked into their core product. Within Kubernetes, a container can rise or fall and Kubernetes will be aware and able to react.  Within the ELB, new instances that you create tuck right where you would expect. Removed instances, too, depart from order when they should. Clients keep connecting, services change, and everything rolls along nicely. 

But, what if you have multiple Kubernetes clusters? You can replicate state across multiple clusters, true, but you cannot discover services that belong to a different cluster. 

What if you need to operate in a medley of different clouds? How do you keep registries highly available? With all these services coming and going all around the world, how do you keep everything monitored? 

Enough questions! Time for some answers.

## The Consul-ar, Diplomat of Services
Now that we understand the depth of our inquiry, we can introduce Consul. Consul answers our key questions through **service discovery**, the **gossip protocol** and  **queries**.

### Ready for Service

To begin, we need to know what Consul considers a service. A service is an API, framework, UI, database, cache, or any other de-coupled, context bound function that an application performs. 

A service resides within a node. A node is a container, VM, or other sort of compute instance that runs a service. In order to register the service contained within a node, you want to have the node running as a Consul agent.

If, for example, you have a node running Rails and Consul, you would want to specify configuration details within `/etc/consul.d/web.json`.  

On a basic level, it might look something like this…

```
{ 
	"service": {
		"name": "web", 
		"tags": ["rails"], 
		"port": 80,
		“check”: {
			“script”: “/usr/local/bin/check_rails.py”,
			“interval”: “10s”
		} 
	}
}
```

A service can be described any way that you see fit. Tags, names, ports, scripts, can all be specified to fit your unique use-case. 

When you run `consul agent -config-dir=/etc/consul.d`, the agent comes alive representing the service details. The details are stored within the clusters’ catalog. It is this information, along with the network location, that other members of the cluster will be able to receive in response from the catalog, and the overall, failure tolerant, distributed registry once it has been populated with enough servers and clients.

A client can can access this data in two different ways, via DNS or the REST API. 

To query via DNS, we use  `dig tag.thing.service.consul`. A response might look like this:

```
$ dig rails.web.service.consul
…
;; ANSWER SECTION:
rails.web.service.consul. 0   IN  A   255.255.255.255
```

If we want to get the port, too:

```
$ dig rails.web.service.consul SRV
…
;; ANSWER SECTION:
rails.web.service.consul. 0   IN  SRV 1 1 80 example.node.dc1.consul.

```

If we wanted to query via the the API using HTTP, we can do that…

```
$ curl http://localhost:port/v1/catalog/service/web
…
[ 
	{
		"Node":"node-example",
		"Address":"255.255.255.255",
		"ServiceID":"web", 
		"ServiceName":"web",
		"ServiceTags":["rails"],
		"ServicePort":80
	}
]
```

In each case, the response that is returned represents our service. But this is just one service. We want to see the distributed, fault-tolerant goodness that manages many services. For that, we need more Consul agents. 

### The Agent: Server or Client

The Consul agent must be designated a client or a server. For resilience, we want _at least_ 3 server agents, to handle one server failure, or 5 server agents, to handle two failures. We can have any number of clients.

When an agent is in client mode, the agent organizes the entire picture of registered services and creates health checks for its node. In our basic example, our `check` key has the location of a python script, `/usr/local/bin/check_rails.py` and an interval that it activates. The script might be hitting localhost to generate a response, for example. This is one facet of failure detection.

When in server mode, the agents do everything the clients do, but perform a few additional functions. The servers govern consensus via the RAFT algorithm; RAFT enables the multi-member state consistency and leadership election required for distributed, large-scale K/V storage.  

Client agents speak to the servers to read or update K/V data using Remote Procedural Calls. The server which receives the call will forward the key to the leader, who will then become eventually consistent with the other server members. In response, the leaders send a blocking RPC call back to each agent, and in so-doing create our intelligent, resilient, aware service registration layer that includes all agents. The servers will also enforce Access Control Lists, if they have been enabled.

Adding services and tracking protocol discourse is fun and all, but what about failures? In particular, what about _partial_ failures. For that, some spicy gossip. 

### Hot Gossip

The real danger zone in microservice architecture lies not in service failure. It lies within partial service failure. A partially failing service is a wretched thing. It reads as online but it is running slow or not running at all. The registry, and you, think things are fine but every client hitting that service is having a poor experience. 

To address this issue, Consul introduces the gossip protocol. The gossip protocol creates a layer of base-line traffic among the servers and the agents. 

The agents talk to the servers, the servers talk to the agents. But it would not be gossip it it were a clear, linear, logical discourse. Agents and servers will talk to other agents and servers on behalf of other members. 

A metaphor to understand this is like a fleet of boats washed up on a sandy shore. Some of the boats have holes in them and are unable to float. To determine which boats have failed, you would not want a single wave splashing each boat. What if the hole was on the _other_ side or the boat was still able to resist to that certain type of splash? To really see which vessel is sea-worthy, we would want to bring in the tide, bring about simulated, realistic, demand, and challenge the boats.

Gossip traffic starts as UDP. If that fails, it tries TCP, then it will try asking on behalf of peers. Failures are determined from the edges, relating in a direct way to service discovery. If the server has failed, it will leave rotation.

Consul does this with a clear and robust log output. The logging output from this discovery process is most useful. As gossip occurs and states change, a centralized log exists that allows you to audit your flow of events.

The gossip protocol seems like an excellent way to keep tabs on a Local Area Network of services, but what happens when you have more than one datacenter? What happens when LAN becomes WAN? Right, right — answers, no more questions!

### Grand Scale

Each Consul cluster contains many nodes made up of server agents and client agents. Consul can run independent clusters. Or, it can weave independent, distributed clusters together. The end result is a unified mesh, wherein a user requests will arrive and then be directed towards the fastest service as determined by round-trip time.

Consider the Gossip protocol, but now relax the timing to account for the packet loss and latency of the WAN. The server agents between datacenters communicate in the same way that server clients and server agents do within an individual cluster. Server agents forward Remote Procedure Call to server agents in other datacenters. 

If we look at the entire thread, consider that a client agent in DC2 updates a Key/Value pair. The client agent in DC2 RPC forwards to the server agent in DC2. That server in DC2 then tells the leader, then synchronizes the others, which in turn informs its respective local agents. This is real neat, but it is just the tip of the iceberg.

Things get  even more delightful when you factor in _prepared queries_. A prepared query allows you to create a query name-space. When the namespace is called, the query runs against Consul, which supplies the results based on the definition. It is pure magic. 

A prepared query is a front for distributed services. If, for example, you wanted to query your distributed Redis slaves, you would `POST` a query template to the `query` endpoint, like so…

```
$ curl -X POST -d \
'{
  "Name":"distributed-redis-slaves",
  "Service": {
    "Service": "redis",
    "Failover": {
      "NearestN": 3 
  },
  "Tags": ["slave"]
  }
}', localhost:port/v1/query
``` 

Earlier, we found that after creating a service it could be queried at the end-point ending in `x.x.service.consul`, such as this: `tag.thing.service.consul`.

Now that we have prepared a query, we can look for all `redis` services with the tag `slave` by querying `distributed-redis-slaves.query.consul`. That is quite a simple query. But it contains great power!

When we further unpack our `POST`, we see that we have specified a `Failover` key containing a `NearestN` key that we have given a value of 3. Our queries will check locally and if no Redis slaves are available then it will try the nearest _3_ datacenters. To make this even more funky, it will do so based on the shortest round-trip time.

With prepared query templates, we can get even deeper. Consider that you have hundreds of services. If you needed to define queries for hundreds of sub-sets, that would be cumbersome…

```
curl -X POST -d \
'{
  "Name": "distributed-redis",
  "Template:" {
    "Type": "name_prefix_match",
    "Regexp": "^distributed-redis-(.*?)-([^\\-]+?)$"
  },
  "Service:" {
    "Service:" "redis-${match(1)}",
    "Failover": {
      "NearestN" : 3,
      "Datacenters": ["dc1","dc2"]
  },
  "OnlyPassing": true,
  "Tags": ["$[match(2)}"]
 }
}', localhost:port/v1/query
```

Any query we make that starts with `distributed-redis`  will be parsed using RegEx. RegEx will pull out your specified strings and perform substitutions. 

Consider that we want to query `distributed-redis-counter-slave.query.consul`. We have a few different Redis instances, but we want a set of slaves that make-up the counting data-store the scoreboard in our web game uses. The RegEx will pull out `counter` and `slave`, then plug them into a regular query. 

You will return the nearest, fastest `redis-counter` service, which is active as specified by making `"OnlyPassing": true` , and which is tagged as  a `slave`. Now, consider that you can consider that you can set-up a catchall: `*.query.consul`. Neat!

When we put **service discovery**, the **gossip protocol**, and **prepared queries** together, we know that service management becomes much more simple. We also know that a fleet of agents can stay inter-connected to detect and respond to failures. Through prepared queries, we see how we would load balance and deliver our services to meet massive demand — we need only expose a DNS or HTTP call in order to wield a battalion of services.

## Summary
The Monolithic need not be criticized for it has served us well. Many early stage companies have done remarkable and wondrous things working on unwieldy Monolithic applications. But at a certain level of scale, a more manageable approach is needed. A plentitude of varied services is essential for high-demand applications.

Microservice architecture is a popular approach. But it is not without important considerations. How are you going to discover services? How are services going to connect across multiple datacentres? How can monitoring and failure management be done in a sane and coherent way? 

To answer these questions, we have Consul. This brief overview dips into the problem areas that Consul is tailored to address. But we left out the growing library of other open-source utilities made by the vibrant Consul community. 

The takeaway is that if you are managing sophisticated and diverse service-based architecture, you should take a deep and thoughtful look at Consul. If you are interesting bringing it into your infrastructure, it is open-source and ready to roll. If you are more interested in a managed offering with automated backups, upgrades, and more intelligent network segmentation, Consul Enterprise could be your ticket.